{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# United States Case Law Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Wrangling__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from casewrangler import CaseWrangler\n",
    "import os, sys, pickle, string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import lucem_illud \n",
    "import gensim #For word2vec, etc\n",
    "import nltk #For stop words and stemmers\n",
    "from nltk.corpus import stopwords #For stopwords\n",
    "import sklearn.metrics.pairwise #For cosine similarity\n",
    "import sklearn.manifold #For T-SNE\n",
    "import sklearn.decomposition #For PCA\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import os.path\n",
    "stop_words_nltk = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw = CaseWrangler()\n",
    "\n",
    "usPath = os.path.abspath('United States-20181204-xml/data/data.jsonl.xz')\n",
    "usCorpus = os.path.abspath('United States-20181204-xml/corpus')\n",
    "usCases = cw.extractCases(usPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scCases = []\n",
    "for case in usCases:\n",
    "    if case['court']['name'] == 'Supreme Court of the United States':\n",
    "        scCases.append(case)\n",
    "        \n",
    "scDF = pd.DataFrame.from_dict(scCases)\n",
    "scDF = scDF.drop(columns=['citations', 'court', 'first_page', 'id', 'jurisdiction', 'last_page', 'name', 'reporter', 'volume'])\n",
    "\n",
    "scDF['decision_date'] = pd.to_datetime(scDF['decision_date'])\n",
    "scDF['docket_number'].replace(regex=True, inplace=True, to_replace=r'[No. ]',value=r'')\n",
    "scDF['docket_number'].replace(regex=True, inplace=True, to_replace=r'[;]',value=r',')\n",
    "\n",
    "scDF['opinions'] = [scDF.casebody[i]['data']['opinions'] for i in scDF.index]\n",
    "scDF = scDF.drop(columns=['casebody'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "allOpinions = []\n",
    "opTypes = ['majority', 'dissent', 'concurrence', 'concurring-in-part-and-dissenting-in-part']\n",
    "\n",
    "for opinions in scDF.opinions:\n",
    "    text = [opinion['text'] for opinion in opinions if opinion['type'] in opTypes]\n",
    "    allOpinions.append(text)\n",
    "    \n",
    "scDF['opinionText'] = allOpinions\n",
    "scDF['opinionText'] = [' '.join(text) for text in allOpinions]\n",
    "# scDF['length'] = scDF.opinionText.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 331443 entries, 0 to 331442\n",
      "Data columns (total 7 columns):\n",
      "decision_date        331443 non-null datetime64[ns]\n",
      "docket_number        331443 non-null object\n",
      "name_abbreviation    331443 non-null object\n",
      "opinions             331443 non-null object\n",
      "opinionText          331443 non-null object\n",
      "docket               331443 non-null object\n",
      "issueArea            327488 non-null float64\n",
      "dtypes: datetime64[ns](1), float64(1), object(5)\n",
      "memory usage: 20.2+ MB\n"
     ]
    }
   ],
   "source": [
    "scdbDF = pd.read_excel('SCDB_2018_02_caseCentered_Docket.xlsx')\n",
    "issueDF = scdbDF[['docket', 'issueArea']]\n",
    "scDF_issue = scDF.merge(issueDF, how='inner', left_on='docket_number', right_on='docket')\n",
    "scDF_issue.info()\n",
    "# civCases = caseDFmerged[caseDFmerged['issueArea'] == 2]\n",
    "# civCases.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Cleaning__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decision_date</th>\n",
       "      <th>docket_number</th>\n",
       "      <th>name_abbreviation</th>\n",
       "      <th>opinionText</th>\n",
       "      <th>issueArea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1977-05-31</td>\n",
       "      <td>76-333</td>\n",
       "      <td>United Air Lines, Inc. v. Evans</td>\n",
       "      <td>mr. justice stevens delivered the opinion of t...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1977-01-25</td>\n",
       "      <td>76-333</td>\n",
       "      <td>United Air Lines, Inc. v. Evans</td>\n",
       "      <td>c. a. 7th cir. certiorari granted, ante, p. 91...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1977-01-17</td>\n",
       "      <td>76-333</td>\n",
       "      <td>United Air Lines, Inc. v. Evans</td>\n",
       "      <td>c. a. 7th cir. certiorari granted, ante, p. 91...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1976-11-01</td>\n",
       "      <td>76-333</td>\n",
       "      <td>United Air Lines, Inc. v. Evans</td>\n",
       "      <td>c. a. 7th cir. certiorari granted.</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1977-05-31</td>\n",
       "      <td>76-777</td>\n",
       "      <td>Connor v. Finch</td>\n",
       "      <td>mr. justice stewart delivered the opinion of t...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  decision_date docket_number                name_abbreviation  \\\n",
       "0    1977-05-31        76-333  United Air Lines, Inc. v. Evans   \n",
       "1    1977-01-25        76-333  United Air Lines, Inc. v. Evans   \n",
       "2    1977-01-17        76-333  United Air Lines, Inc. v. Evans   \n",
       "3    1976-11-01        76-333  United Air Lines, Inc. v. Evans   \n",
       "4    1977-05-31        76-777                  Connor v. Finch   \n",
       "\n",
       "                                         opinionText  issueArea  \n",
       "0  mr. justice stevens delivered the opinion of t...        2.0  \n",
       "1  c. a. 7th cir. certiorari granted, ante, p. 91...        2.0  \n",
       "2  c. a. 7th cir. certiorari granted, ante, p. 91...        2.0  \n",
       "3                 c. a. 7th cir. certiorari granted.        2.0  \n",
       "4  mr. justice stewart delivered the opinion of t...        2.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scDF_issue.opinionText = scDF_issue.opinionText.map(lambda x: x.lower())\n",
    "scDF_issue.opinionText = scDF_issue.opinionText.map(lambda x: x.replace('\\n', ' ')) \n",
    "scDF_issue.opinionText = scDF_issue.opinionText.map(lambda x: x.replace('§', 'section')) \n",
    "scDF_issue.opinionText = scDF_issue.opinionText.map(lambda x: x.replace('united states', 'u_s'))\n",
    "scDF_issue.opinionText = scDF_issue.opinionText.map(lambda x: x.replace(' u. s. ', '_u_s_')) \n",
    "scDF_issue.opinionText = scDF_issue.opinionText.map(lambda x: x.replace(' u.s. ', 'u_s')) \n",
    "scDF_issue.opinionText = scDF_issue.opinionText.map(lambda x: x.replace(' v. ', '_v_')) \n",
    "scDF_issue.opinionText = scDF_issue.opinionText.map(lambda x: x.replace(' f. supp. ', '_f_supp_')) \n",
    "scDF_issue.opinionText = scDF_issue.opinionText.map(lambda x: x.replace(' u. s. c. ', '_u_s_c_'))\n",
    "scDF_issue.opinionText = scDF_issue.opinionText.map(lambda x: x.replace('■', ''))\n",
    "scDF_issue.opinionText = scDF_issue.opinionText.map(lambda x: x.replace('  ', ' '))\n",
    "scDF_issue.opinionText = scDF_issue.opinionText.map(lambda x: x.translate(str.maketrans('', '', string.punctuation))) \n",
    "\n",
    "scDF_issue.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decision_date</th>\n",
       "      <th>docket_number</th>\n",
       "      <th>name_abbreviation</th>\n",
       "      <th>opinionText</th>\n",
       "      <th>issueArea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1977-05-31</td>\n",
       "      <td>76-333</td>\n",
       "      <td>United Air Lines, Inc. v. Evans</td>\n",
       "      <td>mr. justice stevens delivered the opinion of t...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1977-01-25</td>\n",
       "      <td>76-333</td>\n",
       "      <td>United Air Lines, Inc. v. Evans</td>\n",
       "      <td>court of appeal 7th circuit certiorari granted...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1977-01-17</td>\n",
       "      <td>76-333</td>\n",
       "      <td>United Air Lines, Inc. v. Evans</td>\n",
       "      <td>court of appeal 7th circuit certiorari granted...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1976-11-01</td>\n",
       "      <td>76-333</td>\n",
       "      <td>United Air Lines, Inc. v. Evans</td>\n",
       "      <td>court of appeal 7th circuit certiorari granted.</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1977-05-31</td>\n",
       "      <td>76-777</td>\n",
       "      <td>Connor v. Finch</td>\n",
       "      <td>mr. justice stewart delivered the opinion of t...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  decision_date docket_number                name_abbreviation  \\\n",
       "0    1977-05-31        76-333  United Air Lines, Inc. v. Evans   \n",
       "1    1977-01-25        76-333  United Air Lines, Inc. v. Evans   \n",
       "2    1977-01-17        76-333  United Air Lines, Inc. v. Evans   \n",
       "3    1976-11-01        76-333  United Air Lines, Inc. v. Evans   \n",
       "4    1977-05-31        76-777                  Connor v. Finch   \n",
       "\n",
       "                                         opinionText  issueArea  \n",
       "0  mr. justice stevens delivered the opinion of t...        2.0  \n",
       "1  court of appeal 7th circuit certiorari granted...        2.0  \n",
       "2  court of appeal 7th circuit certiorari granted...        2.0  \n",
       "3    court of appeal 7th circuit certiorari granted.        2.0  \n",
       "4  mr. justice stewart delivered the opinion of t...        2.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scDF_issue = pd.read_csv('scDF_by_issue.csv')\n",
    "scDF_issue.opinionText = scDF_issue.opinionText.astype(str)\n",
    "scDF_issue.opinionText = scDF_issue.opinionText.map(lambda x: x.replace('  ', ' '))\n",
    "scDF_issue.opinionText = scDF_issue.opinionText.map(lambda x: x.replace('cir.', 'circuit'))\n",
    "scDF_issue.opinionText = scDF_issue.opinionText.map(lambda x: x.replace('c. a.', 'court of appeal'))\n",
    "scDF_issue.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decision_date</th>\n",
       "      <th>docket_number</th>\n",
       "      <th>name_abbreviation</th>\n",
       "      <th>opinionText</th>\n",
       "      <th>issueArea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1977-05-31</td>\n",
       "      <td>76-333</td>\n",
       "      <td>United Air Lines, Inc. v. Evans</td>\n",
       "      <td>mr justice stevens delivered the opinion of th...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1977-01-25</td>\n",
       "      <td>76-333</td>\n",
       "      <td>United Air Lines, Inc. v. Evans</td>\n",
       "      <td>court of appeal 7th circuit certiorari granted...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1977-01-17</td>\n",
       "      <td>76-333</td>\n",
       "      <td>United Air Lines, Inc. v. Evans</td>\n",
       "      <td>court of appeal 7th circuit certiorari granted...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1976-11-01</td>\n",
       "      <td>76-333</td>\n",
       "      <td>United Air Lines, Inc. v. Evans</td>\n",
       "      <td>court of appeal 7th circuit certiorari granted</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1977-05-31</td>\n",
       "      <td>76-777</td>\n",
       "      <td>Connor v. Finch</td>\n",
       "      <td>mr justice stewart delivered the opinion of th...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  decision_date docket_number                name_abbreviation  \\\n",
       "0    1977-05-31        76-333  United Air Lines, Inc. v. Evans   \n",
       "1    1977-01-25        76-333  United Air Lines, Inc. v. Evans   \n",
       "2    1977-01-17        76-333  United Air Lines, Inc. v. Evans   \n",
       "3    1976-11-01        76-333  United Air Lines, Inc. v. Evans   \n",
       "4    1977-05-31        76-777                  Connor v. Finch   \n",
       "\n",
       "                                         opinionText  issueArea  \n",
       "0  mr justice stevens delivered the opinion of th...        2.0  \n",
       "1  court of appeal 7th circuit certiorari granted...        2.0  \n",
       "2  court of appeal 7th circuit certiorari granted...        2.0  \n",
       "3     court of appeal 7th circuit certiorari granted        2.0  \n",
       "4  mr justice stewart delivered the opinion of th...        2.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scDF_issue.opinionText = scDF_issue.opinionText.map(lambda x: x.translate(str.maketrans('', '', string.punctuation))) \n",
    "# scDF_issue.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scDF_issue['tokenized_text'] = \\\n",
    "    scDF_issue['opinionText'].apply(lambda x: [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])\n",
    "\n",
    "scDF_issue['sent_count'] = scDF_issue['tokenized_text'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Word Embeddings__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 65216 entries, 0 to 204554\n",
      "Data columns (total 7 columns):\n",
      "decision_date        65216 non-null object\n",
      "docket_number        65216 non-null object\n",
      "name_abbreviation    65216 non-null object\n",
      "opinionText          65216 non-null object\n",
      "issueArea            63760 non-null float64\n",
      "tokenized_text       65216 non-null object\n",
      "sent_count           65216 non-null int64\n",
      "dtypes: float64(1), int64(1), object(5)\n",
      "memory usage: 4.0+ MB\n"
     ]
    }
   ],
   "source": [
    "scDF_issue_test = scDF_issue[scDF_issue.sent_count >= 5]\n",
    "scDF_issue_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __(a) CBOW__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scW2V = gensim.models.word2vec.Word2Vec(scDF_issue_test['normalized_tokens'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 100 dimesional vector:\n"
     ]
    }
   ],
   "source": [
    "print(\"A {} dimesional vector:\".format(scW2V['abortion'].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('abortions', 0.7884458303451538),\n",
       " ('postviability', 0.6882057189941406),\n",
       " ('woman', 0.6716547012329102),\n",
       " ('pregnancy', 0.6633925437927246),\n",
       " ('pregnant', 0.6534528732299805),\n",
       " ('childbirth', 0.6517119407653809),\n",
       " ('pregnancies', 0.6327844858169556),\n",
       " ('surgery', 0.6324198246002197),\n",
       " ('fetus', 0.6008843183517456),\n",
       " ('physician', 0.5988410115242004)]"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scW2V.most_similar('abortion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numWords = 150\n",
    "targetWords = scW2V.wv.index2word[numWords]\n",
    "\n",
    "wordsSubMatrix = []\n",
    "for word in targetWords:\n",
    "    wordsSubMatrix.append(scW2V[word])\n",
    "wordsSubMatrix = np.array(wordsSubMatrix)\n",
    "wordsSubMatrix\n",
    "\n",
    "pcaWords = sklearn.decomposition.PCA(n_components = 50).fit(wordsSubMatrix)\n",
    "reducedPCA_data = pcaWords.transform(wordsSubMatrix)\n",
    "#T-SNE is theoretically better, but you should experiment\n",
    "tsneWords = sklearn.manifold.TSNE(n_components = 2, early_exaggeration = 25).fit_transform(reducedPCA_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (15,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_frame_on(False)\n",
    "plt.scatter(tsneWords[:, 0], tsneWords[:, 1], alpha = 0)#Making the points invisible \n",
    "for i, word in enumerate(targetWords):\n",
    "    ax.annotate(word, \n",
    "                (tsneWords[:, 0][i],tsneWords[:, 1][i]), \n",
    "                size =  20 * (numWords - i) / numWords, \n",
    "                alpha = .8 * (numWords - i) / numWords + .2)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __(b) SGNS__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scW2V = gensim.models.word2vec.Word2Vec(scDF_issue_test['tokenized_text'].sum(), min_count=100, workers=8, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'abortion' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-4a97e10b8d4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"A {} dimesional vector:\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscW2V\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'abortion'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Applications/anaconda3/envs/venv_thesis/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1300\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m                 )\n\u001b[0;32m-> 1302\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/venv_thesis/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m   1421\u001b[0m         \u001b[0mRefer\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocumentation\u001b[0m \u001b[0;32mfor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m         \"\"\"\n\u001b[0;32m-> 1423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method will be removed in 4.0.0, use self.wv.__contains__() instead\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/venv_thesis/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/venv_thesis/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'abortion' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "print(\"A {} dimesional vector:\".format(scW2V['abortion'].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('abortions', 0.7884458303451538),\n",
       " ('postviability', 0.6882057189941406),\n",
       " ('woman', 0.6716547012329102),\n",
       " ('pregnancy', 0.6633925437927246),\n",
       " ('pregnant', 0.6534528732299805),\n",
       " ('childbirth', 0.6517119407653809),\n",
       " ('pregnancies', 0.6327844858169556),\n",
       " ('surgery', 0.6324198246002197),\n",
       " ('fetus', 0.6008843183517456),\n",
       " ('physician', 0.5988410115242004)]"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scW2V.most_similar('abortion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/venv_thesis/lib/python3.6/site-packages/sklearn/decomposition/pca.py:423: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance_ = (S ** 2) / (n_samples - 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected n_neighbors > 0. Got 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5b521351dee6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mreducedPCA_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpcaWords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordsSubMatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#T-SNE is theoretically better, but you should experiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtsneWords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanifold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_exaggeration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreducedPCA_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Applications/anaconda3/envs/venv_thesis/lib/python3.6/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    856\u001b[0m             \u001b[0mEmbedding\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdimensional\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m         \"\"\"\n\u001b[0;32m--> 858\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/venv_thesis/lib/python3.6/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, skip_num_points)\u001b[0m\n\u001b[1;32m    715\u001b[0m                                    metric=self.metric)\n\u001b[1;32m    716\u001b[0m             \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m             \u001b[0mknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m             \u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/envs/venv_thesis/lib/python3.6/site-packages/sklearn/neighbors/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'precomputed'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m         \"\"\"\n\u001b[0;32m--> 803\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Applications/anaconda3/envs/venv_thesis/lib/python3.6/site-packages/sklearn/neighbors/base.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    257\u001b[0m                 raise ValueError(\n\u001b[1;32m    258\u001b[0m                     \u001b[0;34m\"Expected n_neighbors > 0. Got %d\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m                 )\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected n_neighbors > 0. Got 0"
     ]
    }
   ],
   "source": [
    "numWords = 50\n",
    "targetWords = scW2V.wv.index2word[numWords]\n",
    "\n",
    "wordsSubMatrix = []\n",
    "for word in targetWords:\n",
    "    wordsSubMatrix.append(scW2V[word])\n",
    "wordsSubMatrix = np.array(wordsSubMatrix)\n",
    "wordsSubMatrix\n",
    "\n",
    "pcaWords = sklearn.decomposition.PCA(n_components = 50).fit(wordsSubMatrix)\n",
    "reducedPCA_data = pcaWords.transform(wordsSubMatrix)\n",
    "#T-SNE is theoretically better, but you should experiment\n",
    "tsneWords = sklearn.manifold.TSNE(n_components = 2, early_exaggeration = 25).fit_transform(reducedPCA_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (15,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_frame_on(False)\n",
    "plt.scatter(tsneWords[:, 0], tsneWords[:, 1], alpha = 0)#Making the points invisible \n",
    "for i, word in enumerate(targetWords):\n",
    "    ax.annotate(word, \n",
    "                (tsneWords[:, 0][i],tsneWords[:, 1][i]), \n",
    "                size =  20 * (numWords - i) / numWords, \n",
    "                alpha = .8 * (numWords - i) / numWords + .2)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Confidence Intervals__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boostrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimatesB=[]\n",
    "for x in range(20):\n",
    "    scW2VB = gensim.models.word2vec.Word2Vec(scDF_filter['normalized_sents'].sample(frac=1.0, replace=True).sum())\n",
    "    try:\n",
    "        estimatesB.append(cos_difference(scW2VB, 'liberty', 'constraint')[0,0])\n",
    "    except KeyError:\n",
    "        #Missing one of the words from the vocab\n",
    "        pass\n",
    "                                                      \n",
    "estimatesB.sort()         \n",
    "estimatesB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The 90% confidence interval for the cosine distance between liberty and constraint is:\\n\",\n",
    "      estimatesB[1], estimatesB[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10\n",
    "sample_indices = np.random.randint(0,n_samples,(len(scDF_filter),))\n",
    "\n",
    "s_k =np.array([])\n",
    "tau_k=np.array([])\n",
    "\n",
    "for i in range(n_samples):\n",
    "    sample_w2v = gensim.models.word2vec.Word2Vec(scDF_filter[sample_indices == i]['normalized_sents'].sum())\n",
    "    try:\n",
    "        #Need to use words present in most samples\n",
    "        s_k = np.append(s_k, cos_difference(sample_w2v, 'liberty', 'constraint')[0,0])\n",
    "    except KeyError:\n",
    "        pass\n",
    "    else:\n",
    "        tau_k = np.append(tau_k, len(scDF_filter[sample_indices == i]))\n",
    "\n",
    "print(s_k)\n",
    "print(tau_k)\n",
    "\n",
    "tau = tau_k.sum()\n",
    "s = s_k.mean()\n",
    "B_k = np.sqrt(tau_k) * s_k-s_k.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The 90% confidence interval for the cosine distance between liberty and constraint is:\\n\",\n",
    "      s-B_k[-2]/np.sqrt(tau), s-B_k[1]/np.sqrt(tau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scW2V.save(\"WORD2VecTest.mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scW2V_loss = gensim.models.word2vec.Word2Vec(size = 100, alpha=0.025, window=5, \n",
    "                                             min_count=5, hs=0, compute_loss = True)\n",
    "\n",
    "scW2V_loss.build_vocab(scDF_filter['normalized_sents'].sum())\n",
    "scW2V_loss.train(scDF_filter['normalized_sents'].sum(), total_examples=scW2V.corpus_count, epochs=1)\n",
    "#Using a list so we can capture every epoch\n",
    "losses = [scW2V_loss.running_training_loss]\n",
    "losses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(19):\n",
    "    scW2V_loss.train(scDF_filter['normalized_sents'].sum(), \n",
    "                     total_examples=senReleasesW2V.corpus_count, epochs=1)\n",
    "    losses.append(scW2V_loss.running_training_loss)\n",
    "    print(\"Done epoch {}\".format(i + 2), end = '\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossesDF = pd.DataFrame({'loss' : losses, 'epoch' : range(len(losses))})\n",
    "lossesDF.plot(y = 'loss', x = 'epoch', logy=False, figsize=(15, 7))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_dims=[]\n",
    "\n",
    "for d in [50,100,150,200,250,300,350,400,450,500, 550, 600, 650, 700, 750]:\n",
    "    scW2V_loss_dims = gensim.models.word2vec.Word2Vec(size = d, alpha=0.025, window=5, \n",
    "                                                      min_count=5, hs=0, compute_loss = True)\n",
    "    scW2V_loss_dims.build_vocab(scDF_filter['normalized_sents'].sum())\n",
    "    scW2V_loss_dims.train(scDF_filter['normalized_sents'].sum(), \n",
    "                     total_examples = scW2V.corpus_count, \n",
    "                     epochs=7)\n",
    "    scW2V_loss_dims.train(scDF_filter['normalized_sents'].sum(), \n",
    "                     total_examples = scW2V.corpus_count, \n",
    "                     epochs=1)\n",
    "    \n",
    "    losses_dims.append(scW2V_loss_dims.running_training_loss/(10+d*10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_dimsDF = pandas.DataFrame({'loss' : losses_dims, 'dimensions' : [50,100,150,200,250,300,350,400,450,500,550,600,650,700,750]})\n",
    "losses_dimsDF.plot(y = 'loss', x = 'dimensions', logy=False, figsize=(15, 7))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Linguistic Change__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcSynNorm(model):\n",
    "    \"\"\"since syn0norm is now depricated\"\"\"\n",
    "    return (model.wv.syn0 / np.sqrt((model.wv.syn0 ** 2).sum(-1))[..., np.newaxis]).astype(np.float32)\n",
    "\n",
    "def smartProcrustesAlignGensim(base_embed, other_embed, words=None):\n",
    "    \"\"\"Procrustes align two gensim word2vec models (to allow for comparison between same word across models).\n",
    "    Code ported from HistWords <https://github.com/williamleif/histwords> by William Hamilton <wleif@stanford.edu>.\n",
    "    (With help from William. Thank you!)\n",
    "    First, intersect the vocabularies (see 'intersectionAlignGensim' documentation).\n",
    "    Then do the alignment on the other_embed model.\n",
    "    Replace the other_embed model's syn0 and syn0norm numpy matrices with the aligned version.\n",
    "    Return other_embed.\n",
    "    If `words` is set, intersect the two models' vocabulary with the vocabulary in words (see 'intersectionAlignGensim' documentation).\n",
    "    \"\"\"\n",
    "    base_embed = copy.copy(base_embed)\n",
    "    other_embed = copy.copy(other_embed)\n",
    "    # make sure vocabulary and indices are aligned\n",
    "    in_base_embed, in_other_embed = intersectionAlignGensim(base_embed, other_embed, words=words)\n",
    "\n",
    "    # get the embedding matrices\n",
    "    base_vecs = calcSynNorm(in_base_embed)\n",
    "    other_vecs = calcSynNorm(in_other_embed)\n",
    "\n",
    "    # just a matrix dot product with numpy\n",
    "    m = other_vecs.T.dot(base_vecs) \n",
    "    # SVD method from numpy\n",
    "    u, _, v = np.linalg.svd(m)\n",
    "    # another matrix operation\n",
    "    ortho = u.dot(v) \n",
    "    # Replace original array with modified one\n",
    "    # i.e. multiplying the embedding matrix (syn0norm)by \"ortho\"\n",
    "    other_embed.wv.syn0norm = other_embed.wv.syn0 = (calcSynNorm(other_embed)).dot(ortho)\n",
    "    return other_embed\n",
    "    \n",
    "def intersectionAlignGensim(m1,m2, words=None):\n",
    "    \"\"\"\n",
    "    Intersect two gensim word2vec models, m1 and m2.\n",
    "    Only the shared vocabulary between them is kept.\n",
    "    If 'words' is set (as list or set), then the vocabulary is intersected with this list as well.\n",
    "    Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).\n",
    "    These indices correspond to the new syn0 and syn0norm objects in both gensim models:\n",
    "        -- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0\n",
    "        -- you can find the index of any word on the .index2word list: model.index2word.index(word) => 2\n",
    "    The .vocab dictionary is also updated for each model, preserving the count but updating the index.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the vocab for each model\n",
    "    vocab_m1 = set(m1.wv.vocab.keys())\n",
    "    vocab_m2 = set(m2.wv.vocab.keys())\n",
    "\n",
    "    # Find the common vocabulary\n",
    "    common_vocab = vocab_m1&vocab_m2\n",
    "    if words: common_vocab&=set(words)\n",
    "\n",
    "    # If no alignment necessary because vocab is identical...\n",
    "    if not vocab_m1-common_vocab and not vocab_m2-common_vocab:\n",
    "        return (m1,m2)\n",
    "\n",
    "    # Otherwise sort by frequency (summed for both)\n",
    "    common_vocab = list(common_vocab)\n",
    "    common_vocab.sort(key=lambda w: m1.wv.vocab[w].count + m2.wv.vocab[w].count,reverse=True)\n",
    "\n",
    "    # Then for each model...\n",
    "    for m in [m1,m2]:\n",
    "        # Replace old syn0norm array with new one (with common vocab)\n",
    "        indices = [m.wv.vocab[w].index for w in common_vocab]\n",
    "        old_arr = calcSynNorm(m)\n",
    "        new_arr = np.array([old_arr[index] for index in indices])\n",
    "        m.wv.syn0norm = m.wv.syn0 = new_arr\n",
    "\n",
    "        # Replace old vocab dictionary with new one (with common vocab)\n",
    "        # and old index2word with new one\n",
    "        m.index2word = common_vocab\n",
    "        old_vocab = m.wv.vocab\n",
    "        new_vocab = {}\n",
    "        for new_index,word in enumerate(common_vocab):\n",
    "            old_vocab_obj=old_vocab[word]\n",
    "            new_vocab[word] = gensim.models.word2vec.Vocab(index=new_index, count=old_vocab_obj.count)\n",
    "        m.wv.vocab = new_vocab\n",
    "\n",
    "    return (m1,m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareModels(df, category, sort = True):\n",
    "    \"\"\"If you are using time as your category sorting is important\"\"\"\n",
    "    embeddings_raw = {}\n",
    "    cats = sorted(set(df[category]))\n",
    "    for cat in cats:\n",
    "        #This can take a while\n",
    "        print(\"Embedding {}\".format(cat), end = '\\r')\n",
    "        subsetDF = df[df[category] == cat]\n",
    "        #You might want to change the W2V parameters\n",
    "        embeddings_raw[cat] = gensim.models.word2vec.Word2Vec(subsetDF['normalized_sents'].sum())\n",
    "    #These are much quicker\n",
    "    embeddings_aligned = {}\n",
    "    for catOuter in cats:\n",
    "        embeddings_aligned[catOuter] = [embeddings_raw[catOuter]]\n",
    "        for catInner in cats:\n",
    "            embeddings_aligned[catOuter].append(smartProcrustesAlignGensim(embeddings_aligned[catOuter][-1], embeddings_raw[catInner]))\n",
    "    return embeddings_raw, embeddings_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinionDF = copy.deepcopy(scDF_issue)\n",
    "opinionDF['year'] = opinionDF.decision_date.year\n",
    "rawEmbeddings, comparedEmbeddings = compareModels(opinionDF, 'year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDivergenceDF(word, embeddingsDict):\n",
    "    dists = []\n",
    "    cats = sorted(set(embeddingsDict.keys()))\n",
    "    dists = {}\n",
    "    for cat in cats:\n",
    "        dists[cat] = []\n",
    "        for embed in embeddingsDict[cat][1:]:\n",
    "            dists[cat].append(np.abs(1 - sklearn.metrics.pairwise.cosine_similarity(embeddingsDict[cat][0][word],\n",
    "                                                                             embed[word])[0,0]))\n",
    "    return pandas.DataFrame(dists, index = cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetWord = 'abortion'\n",
    "\n",
    "pltDF = getDivergenceDF(targetWord, comparedEmbeddings)\n",
    "fig, ax = plt.subplots(figsize = (15, 7))\n",
    "seaborn.heatmap(pltDF, ax = ax, annot = False) #set annot True for a lot more information\n",
    "ax.set_xlabel(\"Starting year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_title(\"Yearly linguistic change for: '{}'\".format(targetWord))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findDiverence(word, embeddingsDict):\n",
    "    cats = sorted(set(embeddingsDict.keys()))\n",
    "    \n",
    "    dists = []\n",
    "    for embed in embeddingsDict[cats[0]][1:]:\n",
    "        dists.append(1 - sklearn.metrics.pairwise.cosine_similarity(embeddingsDict[cats[0]][0][word], embed[word])[0,0])\n",
    "    return sum(dists)\n",
    "\n",
    "def findMostDivergent(embeddingsDict):\n",
    "    words = []\n",
    "    for embeds in embeddingsDict.values():\n",
    "        for embed in embeds:\n",
    "            words += list(embed.wv.vocab.keys())\n",
    "    words = set(words)\n",
    "    print(\"Found {} words to compare\".format(len(words)))\n",
    "    return sorted([(w, findDiverence(w, embeddingsDict)) for w in words], key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDivergences = findMostDivergent(comparedEmbeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Divergent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDivergences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetWord = wordDivergences[0][0]\n",
    "\n",
    "pltDF = getDivergenceDF(targetWord, comparedEmbeddings)\n",
    "fig, ax = plt.subplots(figsize = (10, 7))\n",
    "seaborn.heatmap(pltDF, ax = ax, annot = False) #set annot True for a lot more information\n",
    "ax.set_xlabel(\"Starting year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_title(\"Yearly linguistic change for: '{}'\".format(targetWord))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Divergent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDivergences[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetWord = wordDivergences[-1][0]\n",
    "\n",
    "pltDF = getDivergenceDF(targetWord, comparedEmbeddings)\n",
    "fig, ax = plt.subplots(figsize = (10, 7))\n",
    "seaborn.heatmap(pltDF, ax = ax, annot = False) #set annot True for a lot more information\n",
    "ax.set_xlabel(\"Starting year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_title(\"Yearly linguistic change for: '{}'\".format(targetWord))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
